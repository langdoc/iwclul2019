---
title: "Some title"
author: "authors"
date: "11/9/2018"
output: html_document
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

```{r}
library(tidyverse)
```


This article describes the tests conducted recently in Kone Foundation funded IKDP-2 project on developing an OCR system for Unified Northern Alphabet, a writing system used a period of time for several languages spoken in Northern areas of Soviet Union. Besides exploring the performance of developed OCR systems, the study has also used crowdsourcing by connecting related tasks into Google Code-In project that introduces open source software to high school students. In the similar vein of effective resource use, the materials used were also selected so that the OCR ground truth texts can form a parallel corpus in the languages tested. Last, but not least, the study presents in appendix the complete list of characters used within Unified Northern Alphabet 

The complete list of languages for which the Unified Northern Alphabet was used is available as Appendix 1. Further studies should look into variability of characters over there, and also test the presented OCR models into that data. Further nationally coordinates projects that produce license-clear Public Domain data, on the footprints of Fenno-Ugrica, would be very beneficial for our cause.

The OCR systems are known to be rather writing system and font specific, instead of language specific. The OCR systems occasionally even display sort of resistence to the attempts to integrate more sophisticated language models (Silfverberg et al). The research question in our study emerges from this situation, and we evaluate whether the OCR system developed on multiple languages performs better or worse than a system that has seen data only from one language. 

## History of Unified Northern Alphabet

The Unified Northern Alphabet was developed for 16 Northern languages in the late 1920s, and taken into use in 1930. It is connected to the Latinization efforts of the time, where a large number of languages spoken in Soviet Union received new Latin based orthographies. In principle UNA is similar to these alphabets, with the difference that the unified base character system is extended for various languages with the use of diacritics and line throughs. On the other hand, for the smaller northern languages UNA was the first effort to create an alphabet, whereas for other languages the Latin scripts replaced the systems earlier in use.

In this process a large number of textbooks and dictionaries were published (Grenoble 2003: 164). Since these books were printed in Saint Petersburg and clearly designed using common materials, they are very close to one another in their content and style. The reason that these materials were intended to be used in creating the literacy among these peoples explains that there are no translations of the same books in larger languages of the Soviet Union.

The Latinization process in Soviet Union started in early 1920s, and was led by Islamic populations which had previously used Arabic script (Grenoble 2003: 49).



## Unified Northern Alphabet

## Experiment design

For the sake of reproduceability, the whole study can be repeated by cloning the repository and running the makefile. It is assumed that Ocropy (**version**) <!-- this needs to be added-->
is installed prior to running the experiments. For additional clarity the individual scripts that repeat the different steps are also described below.

Ground Truth package for Unified Northern Alphabet was developed by Partanen & Rießler in 2019 (citation here). Our study uses a sample from the package's version 1.0, which is available in GitHub. The sampling was done automatically with an R script `1_split_train_and_test.R`. This distributes the lines and matching images into train and test folders.

The model training is done with Ocropy, and this process can be replicated by running Shell script `2_train_mixed_model.sh`. The Kildin Saami model can be trained by running Shell script `3_train_kildin_model.sh`. All models are trained for 100 epochs.

The model evaluation is done with script `4_evaluate.R`.

The languages involved in this study are Kildin Saami, Northern Selkup, Tundra Nenets and Northern Mansi. Two different OCR models are trained. One is trained on all four languages in equal proportions, and the other just with data from Kildin Saami. 

The purpose is to compare the models' performance in mixed and monolingual settings. This, potentially, answers to the question how much of OCR model is language specific and how much the model actually generalizes across languages. The model behaviour is analysed with the potential use with other languages for which this writing system is used in mind.

We use 

The Ground Truth corpus contains bit over 1000 lines, so the mixed training set contains 200 lines, making all together 800 lines. The test set contains 50 lines per language, all together 200 lines. 

Besides this there is a comparative training set for Kildin Saami which contains 400 lines alone. The test sets have 100 lines per language.

The result of multimodel and Kildin-only model are compared against individual languages, with particular attention to whether Kildin model performs significantly better or worse on Kildin test data compared to the multimodel.

It is inevitable that the model will not be able to recognize the characters it hasn't seen.

ģ should be used only in Itelmen, but it occurs also in Kildin. What are typos and what aren't?

The books used contain some headlines that are in very unusual font, which would be only sporadically covered in train and test datasets. For this reason these lines are not used. Ground Truth package's metadata is used to distinguish these lines.

## Training

```{r}
scores <- dir("evaluation/", full.names = TRUE, recursive = TRUE) %>%
  map_df(~ {
    read_tsv(.x, 
         col_names = c("errors", "characters", "line"), 
         col_types = c("iic")) %>%
  mutate(lang = str_extract(line, "(mns|sel|sjd|yrk)")) %>%
  mutate(epoch = str_extract(.x, "\\d{8}") %>% as.numeric()) %>%
  mutate(score = errors / characters) %>%
  mutate(experiment = str_extract(.x, "(mixed|sjd)"))
  }
)
```

Since the mixed model was trained on data from several languages, we can also evaluate the development across the training process by language. 


```{r}
scores %>% 
  filter(experiment == "mixed") %>%
  group_by(epoch, lang) %>%
  summarise(error_mean = mean(score)) %>%
  ggplot(data = ., aes(x = epoch, y = error_mean, color = lang)) +
  geom_line()
```

```{r}
scores %>% 
  filter(experiment == "sjd") %>%
  group_by(epoch) %>%
  summarise(error_mean = mean(score)) %>%
  ggplot(data = ., aes(x = epoch, y = error_mean)) +
  geom_line()
```

Both models have converged into a very high accuracy by 10,000 epochs. For production purposes running the training even longer could be reasonable.

## Results

Since the Kildin Saami model has never seen the characters not used in Kildin Saami, it can be assumed that it performs poorly with data from other languages. This is indeed the case, as plot below summarises. With the mixed test set Kildin Saami model has an error rate of 9%.

In a monolingual Kildin Saami scenario Kildin Saami model outperforms the mixed model, but the difference is only `r 0.02034-0.01469` percents. When compared in context, the similarity of performance in both models when used with Kildin Saami data is even more striking. In both tests Mansi accuracy is the best, which can be argued is caused by small amount of characters only present in Mansi. 

```{r}
tribble(~model, ~test, ~score,
        "mixed",  "mixed", 0.02678,
        "mixed", "mns", 0.01272,
        "mixed", "sel", 0.01610,
        "mixed", "sjd", 0.05372,
        "mixed", "yrk", 0.02887,
        "sjd", "mixed", 0.0912,
        "sjd", "mns", 0.04008,
        "sjd", "sel", 0.12224,
        "sjd", "sjd", 0.05151,
        "sjd", "yrk",  0.14189) %>%
  ggplot(data = ., aes(x = model, y = score, color = test)) +
  geom_point(size = 4)
```

```{r}

get_char_ratio <- function(directory){
    dir(directory, pattern = "gt.txt", full.names = TRUE) %>%
    map_chr(~ read_file(.x)) %>% str_split('') %>% unlist() %>%
    tibble(char = .) %>%
    filter(! str_detect(char, "( |\\n)")) %>%
    mutate(total = n()) %>%
    add_count(char) %>%
    mutate(ratio = n / total) %>%
    arrange(desc(ratio)) %>%
    distinct(char, total, n, ratio) %>%
    mutate(lang = str_extract(directory, "(mns|sel|sjd|yrk)"))
}

char_data <- c("test/mns/", "test/sel/", "test/sjd/", "test/yrk/") %>%
  map(get_char_ratio) %>%
  map(arrange, char) %>%
  bind_rows()

```


In similar fashion the very high error rate in Tundra Nenets can be explained by presence of characters such as `w` and `ꜧ`, latter of which is among the most common characters in Tundra Nenets, but which the Kildin Saami model has never seen. In the same vein the character set used in Tundra Nenets is particularly large, as seen from the table below.

```{r}
char_data %>% group_by(lang) %>% summarise(chars = n()) %>% knitr::kable()
```

The whole character set used is presented below.

```{r}
full_join(char_data %>% 
            filter(lang == "mns") %>% 
            rename(mns = lang) %>%
            select(char, mns),
          char_data %>% 
            filter(lang == "sjd") %>% 
            rename(sjd = lang) %>%
            select(char, sjd)
          ) %>%
  full_join(char_data %>% 
            filter(lang == "sel") %>% 
            rename(sel = lang) %>%
            select(char, sel)
            ) %>% 
    full_join(char_data %>% 
            filter(lang == "yrk") %>% 
            rename(yrk = lang) %>%
            select(char, yrk)) %>%
  arrange(char) %>%
  mutate_all(funs(ifelse(is.na(.), '', .))) %>%
  knitr::kable()
```

Since the models are trained in extremely narrow domain, it comes to mind whether the results are truly generalizable into any other context. Ideally additional tests could be carried out with similar text, but which wouldn't be directly related. 



## Conclusions

One recommendation that arises from our work is that training an OCR model can be done very easily with the current technology. Fast iterative process where the first model is trained with very small data, and used to create little bit larger dataset, on which the same procedure is repeated, appears to be a very effective and effortless approach.

## Further work

The parallel texts should be transformed into contemporary orthographies, and could be employed in variety of purposes. For example, bootstrapping embryonic treebanks with Universal Dependencies project could be very interesting way to make improve the digital infrastructure of these languages in a rather dramatic and visible way.